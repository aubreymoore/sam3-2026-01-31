{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe82903",
   "metadata": {},
   "source": [
    "# processing_videos.ipynb\n",
    "\n",
    "YouTube tutorial: https://docs.ultralytics.com/models/sam-3/\n",
    "\n",
    "Associated code: https://drive.google.com/file/d/1mjRy6VmHke95zdkaCmR_RWvPP4Hnba7q/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fe6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.sam import SAM3VideoSemanticPredictor\n",
    "import glob\n",
    "from icecream import ic\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fa501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def delete_results_from_gpu_memory():\n",
    "    \"\"\"\n",
    "    Explicitly manages memory after processing each image to prevent running out of GPU memory\n",
    "    \"\"\"\n",
    "    global results\n",
    "    del results\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache() # Clears unoccupied cached memory\n",
    "\n",
    "# Usage example:\n",
    "    \n",
    "delete_results_from_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb59df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a short video from the Efate roadside survey\n",
    "\n",
    "output_file = 'roadside.mp4'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "\n",
    "    # get paths to 100 images starting with '20251129_152106.jpg' (a good example)\n",
    "    good_example = '20251129_152106.jpg'\n",
    "    image_dir = '/home/aubrey/Desktop/Efate2025/original_images'\n",
    "    image_paths = sorted(glob.glob(f'{image_dir}/*.jpg'))\n",
    "    good_example_index = image_paths.index(f'{image_dir}/{good_example}')\n",
    "    ic(good_example_index)\n",
    "    ic(len(image_paths))\n",
    "    image_paths = image_paths[good_example_index: good_example_index+100]\n",
    "    ic(len(image_paths))\n",
    "\n",
    "    # Create a VideoWriter object to save the video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Specify the codec for the output video file\n",
    "    video = cv2.VideoWriter(output_file, fourcc, 1.0, (1920, 1080))\n",
    "\n",
    "    # Iterate over each image and write it to the video\n",
    "    for image_path in image_paths:\n",
    "        # image_path = os.path.join(input_folder, image_file)\n",
    "        frame = cv2.imread(image_path)\n",
    "        video.write(frame)\n",
    "\n",
    "    # Release the video writer and close the video file\n",
    "\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09fdd3b7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# Initialize semantic video predictor\n",
    "overrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", imgsz=640, model=\"sam3.pt\", half=True, save=True)\n",
    "predictor = SAM3VideoSemanticPredictor(overrides=overrides)\n",
    "\n",
    "# Track concepts using text prompts\n",
    "results = predictor(source=\"path/to/video.mp4\", text=[\"person\", \"bicycle\"], stream=True)\n",
    "\n",
    "# Process results\n",
    "for r in results:\n",
    "    r.show()  # Display frame with tracked objects\n",
    "\n",
    "# # Alternative: Track with bounding box prompts\n",
    "# results = predictor(\n",
    "#     source=\"path/to/video.mp4\",\n",
    "#     bboxes=[[864, 383, 975, 620], [705, 229, 782, 402]],\n",
    "#     labels=[1, 1],  # Positive labels\n",
    "#     stream=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9acbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAM3 predictor with configuration\n",
    "overrides = dict(\n",
    "    conf=0.25,              # confidence threshold\n",
    "    show_conf=False,        # Enable/disable confidence display\n",
    "    task=\"segment\",         # task i.e. segment\n",
    "    mode=\"predict\",         # mode i.e. predict\n",
    "    model=\"sam3.pt\",        # model file = sam3.pt\n",
    "    half=True,              # Use FP16 for faster inference on GPU.\n",
    "    # device='cpu',\n",
    "    imgsz=1932,  # Adjusted image size from 1920 to meet stride 14 requirement\n",
    "    batch=1,\n",
    ")\n",
    "predictor = SAM3VideoSemanticPredictor(overrides=overrides)\n",
    "results = predictor(source=output_file, text=[\"coconut palm tree\"], stream=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3-2026-01-31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
